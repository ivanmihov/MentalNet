{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"TPU","colab":{"name":"model_stellargraph_DGCNN_colab.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"76415a93"},"source":["# Imports"],"id":"76415a93"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d2b3fe6","executionInfo":{"status":"ok","timestamp":1637014834757,"user_tz":480,"elapsed":25,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"f0752b89-109c-4360-d5bf-0850f0f65e08"},"source":["from platform import python_version\n","print(python_version())"],"id":"5d2b3fe6","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["3.7.12\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73047a93","executionInfo":{"status":"ok","timestamp":1637014837558,"user_tz":480,"elapsed":2809,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"011d84fa-a400-4578-f232-b03643c18c55"},"source":["import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())\n","\n","# gpus = tf.config.experimental.list_physical_devices('GPU')\n","# for gpu in gpus:\n","#     tf.config.experimental.set_memory_growth(gpu, True)\n","    \n","# print(tf.config.list_physical_devices())"],"id":"73047a93","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 16116794567552834832\n","xla_global_id: -1\n","]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b765f08","executionInfo":{"status":"ok","timestamp":1637014837559,"user_tz":480,"elapsed":13,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"3dc6f841-c07b-4285-c79c-4c8ec1d5ace1"},"source":["print(tf.__version__)"],"id":"7b765f08","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2.7.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d02f1e2c","executionInfo":{"status":"ok","timestamp":1637014848068,"user_tz":480,"elapsed":10517,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"f0d5b32b-3e8c-4e81-9da7-2115712855b5"},"source":["import os, sys\n","import pickle\n","\n","import pandas as pd\n","# import modin.pandas as pd\n","\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Dropout, concatenate\n","from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import binary_crossentropy\n","from tensorflow.keras.utils import plot_model\n","\n","from sklearn import model_selection\n","from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n","from sklearn.metrics import precision_score, recall_score\n","\n","from IPython.display import display\n","\n","# import pkg_resources\n","# from packaging.version import Version\n","# if Version(pkg_resources.get_distribution('tables').version) < Version('3.6.1'):\n","!pip install --upgrade tables\n","\n","try:\n","    import stellargraph as sg\n","except ImportError as e:\n","    !pip install stellargraph\n","    import stellargraph as sg\n","    \n","try:\n","    sg.utils.validate_notebook_version(\"1.2.1\")\n","except AttributeError:\n","    raise ValueError(\n","        f\"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>.\"\n","    ) from None\n","\n","from stellargraph import StellarGraph\n","from stellargraph import IndexedArray\n","from stellargraph.mapper import PaddedGraphGenerator, FullBatchNodeGenerator\n","from stellargraph.layer import GCNSupervisedGraphClassification, DeepGraphCNN, GAT\n","import networkx as nx\n","from networkx import draw_networkx"],"id":"d02f1e2c","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (3.4.4)\n","Collecting tables\n","  Downloading tables-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /usr/local/lib/python3.7/dist-packages (from tables) (1.19.5)\n","Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables) (2.7.3)\n","Installing collected packages: tables\n","  Attempting uninstall: tables\n","    Found existing installation: tables 3.4.4\n","    Uninstalling tables-3.4.4:\n","      Successfully uninstalled tables-3.4.4\n","Successfully installed tables-3.6.1\n","Collecting stellargraph\n","  Downloading stellargraph-1.2.1-py3-none-any.whl (435 kB)\n","\u001b[K     |████████████████████████████████| 435 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.0.1)\n","Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.19.5)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.2.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.4.1)\n","Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.7.0)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (1.1.5)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (2.6.3)\n","Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from stellargraph) (3.6.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->stellargraph) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->stellargraph) (0.11.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->stellargraph) (2018.9)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->stellargraph) (1.1.0)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.4.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.1.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.12.0)\n","Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.6.3)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.10.0.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.41.1)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (3.17.3)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.1.2)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (12.0.0)\n","Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.7.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.37.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (0.22.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (1.13.3)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->stellargraph) (2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->stellargraph) (1.5.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.0.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (4.8.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->stellargraph) (3.6.0)\n","Installing collected packages: stellargraph\n","Successfully installed stellargraph-1.2.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"1a7d3caf"},"source":["# Contants"],"id":"1a7d3caf"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b07439b","executionInfo":{"status":"ok","timestamp":1637014869281,"user_tz":480,"elapsed":21222,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"5e1de239-4a2d-43ee-a37b-5200bd548c25"},"source":["# We ignore relationships with no connections\n","min_connections = 1\n","\n","# Storage Paths\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    storage_path = \"/content/drive/MyDrive/ms_proj/\"\n","else:\n","    storage_path = \"/Users/ivan/Google Drive/ms_proj/\"\n","\n","graph_model_stats_file = os.path.join(storage_path, 'graph_model_stats.pickle')\n","\n","def defineGlobalConstants(num_friends_local = 3, num_tweets_local = 10):\n","  df_storage_path = os.path.join(storage_path, \"tweets_dfs\")\n","  df_storage_path_friends = os.path.join(df_storage_path, str(num_friends_local), str(num_tweets_local))\n","  combined_train_test_users_path = os.path.join(storage_path, \"combined_train_test_users\", str(num_friends_local), str(num_tweets_local))\n","\n","  # # Stats\n","  # global hdf_stats_depressed_path\n","  # hdf_stats_depressed_path = os.path.join(df_storage_path_friends, \"stats_depressed.h5\")\n","  # global hdf_stats_control_path\n","  # hdf_stats_control_path = os.path.join(df_storage_path_friends, \"stats_control.h5\")\n","  # global hdf_replies_key\n","  # hdf_replies_key = \"replies\"\n","  # global hdf_mentions_key\n","  # hdf_mentions_key = \"mentions\"\n","  # global hdf_quote_tweets_key\n","  # hdf_quote_tweets_key = \"quote_tweets\"\n","\n","  global hdf_stats_reply_path\n","  hdf_stats_reply_path = os.path.join(df_storage_path_friends, \"stats_reply.h5\")\n","  global hdf_stats_mention_path\n","  hdf_stats_mention_path = os.path.join(df_storage_path_friends, \"stats_mention.h5\")\n","  global hdf_stats_quote_tweets_path\n","  hdf_stats_quote_tweets_path = os.path.join(df_storage_path_friends, \"stats_quote_tweets.h5\")\n","  global hdf_stats_key\n","  hdf_stats_key = \"stats\"\n","\n","  # Train/Test Users\n","  global combined_users_key\n","  combined_users_key = \"combined_users\"\n","  global combined_train_users_path\n","  combined_train_users_path = os.path.join(combined_train_test_users_path, \"train_users.h5\")\n","  global combined_test_users_path\n","  combined_test_users_path = os.path.join(combined_train_test_users_path, \"test_users.h5\")\n","\n","  # Encoded tweets DFs\n","  global hdf_encoded_tweets_path\n","  hdf_encoded_tweets_path = os.path.join(df_storage_path_friends, \"encoded\")\n","  global hdf_encoded_tweets_key\n","  hdf_encoded_tweets_key = \"tweets\""],"id":"4b07439b","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"1c32b6a7"},"source":["# Helper Functions"],"id":"1c32b6a7"},{"cell_type":"code","metadata":{"id":"1c2f7f52","executionInfo":{"status":"ok","timestamp":1637014869284,"user_tz":480,"elapsed":11,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["# Combine multiple dataframes\n","def combineDfs(dfs: list, random = True):\n","    frame_combined = pd.concat(dfs, ignore_index=True)\n","    # frame_combined.reset_index(inplace=True)\n","    # frame_combined.drop(['index'], axis=1, inplace=True)\n","    if random:\n","        return frame_combined.sample(frac = 1)\n","    else:\n","        return frame_combined\n","    \n","# def normalizeDict(d, normalized_low=0, normalized_high=1):\n","#     data_low = min(d.values())\n","#     data_high = max(d.values())\n","    \n","#     if data_low == data_high:\n","#         return d\n","    \n","#     return {key:((value - data_low) / (data_high - data_low)) * (normalized_high - normalized_low) + normalized_low for key,value in d.items()}\n","\n","def normalizeDict(d, target=1.0):\n","    raw = sum(d.values())\n","    factor = target/raw\n","    return {key:value*factor for key,value in d.items()}\n","       \n","def loadCombinedUsers():\n","    if os.path.isfile(combined_train_users_path) and os.path.isfile(combined_test_users_path):\n","        print(\"Found saved train/test users in path: {}. Loading them...\".format(combined_train_users_path))\n","        x_train_users = pd.read_hdf(combined_train_users_path, key=combined_users_key)\n","        x_test_users = pd.read_hdf(combined_test_users_path, key=combined_users_key)\n","        return x_train_users, x_test_users\n","    else:\n","        print(\"No train/test users found in path: {}. Run the main notebook\".format(combined_train_users_path))\n","    \n","def genEncodedTweets(userOrFriend, lstm_layers = 1, word_embeddings = 100, user_embeddings = 200):   \n","    # load the dict from\n","    saved_path = os.path.join(\n","        hdf_encoded_tweets_path,\n","        userOrFriend + \"_\"  + str(lstm_layers) + \"_\" + str(word_embeddings) + \"_\" + str(user_embeddings)+\".pickle\")\n","    \n","    # If we have a saved file, we load the encoded tweets from it\n","    if os.path.isfile(saved_path):\n","        print(\"Found saved {} encoded tweets in path: {}. Loading them...\".format(userOrFriend, saved_path))\n","        encoded_tweets_dict = pickle.load( open( saved_path, \"rb\" ) )\n","        return encoded_tweets_dict\n","    else:\n","        print(\"No encoded tweets found in path: {}. Run the main notebook\".format(saved_path))\n","\n","# Plot an ROC. pred - the predictions, y - the expected output.\n","def plot_roc(pred,y):\n","    fpr, tpr, thresholds = roc_curve(y, pred)\n","    roc_auc = auc(fpr, tpr)\n","\n","    plt.figure()\n","    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","    return roc_auc\n","\n","# Plot a confusion matrix.\n","# cm is the confusion matrix, names are the names of the classes.\n","def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(names))\n","    plt.xticks(tick_marks, names, rotation=45)\n","    plt.yticks(tick_marks, names)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def predToBinPred(pred, threshold = 0.5):\n","  return [1 if p > threshold else 0 for p in pred]\n","\n","# Function to evaluate\n","def evaluate_preds(true, pred):\n","    auc = roc_auc_score(true, pred)\n","    pr = average_precision_score(true, pred)\n","    bin_pred = predToBinPred(pred)\n","    f_score = f1_score(true, bin_pred)\n","    print('ROC AUC:', auc)\n","    print('PR AUC:', pr)\n","    print('F1 score:', f_score)\n","    print(confusion_matrix(true, bin_pred, normalize='true'))\n","    \n","    return auc, pr, f_score"],"id":"1c2f7f52","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3827d81c"},"source":["# Helper Graph Functions"],"id":"3827d81c"},{"cell_type":"code","metadata":{"id":"3eb18e08","executionInfo":{"status":"ok","timestamp":1637014869892,"user_tz":480,"elapsed":617,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def genNodesForGraph(user_id, graph_type):\n","    if graph_type == 'replies':\n","        row = df_replies_stats.loc[df_replies_stats['user_id'] == user_id]\n","    elif graph_type == 'mentions':\n","        row = df_mentions_stats.loc[df_mentions_stats['user_id'] == user_id]\n","    elif graph_type == 'quote_tweets':\n","        row = df_quote_tweets_stats.loc[df_quote_tweets_stats['user_id'] == user_id]\n","    else:\n","        print('Invalid graphType specified: ' + graph_type)\n","        return\n","    \n","    stat_list = row[graph_type].values[0]\n","    user_node = {}\n","    user_node_normalized = {}\n","    friend_nodes = {}\n","    \n","    for stat in stat_list:\n","        friend_nodes[stat.get('friend_id')] = stat.get(graph_type)\n","    \n","    user_node[user_id] = friend_nodes\n","    \n","    friend_nodes_normalized = normalizeDict(friend_nodes)\n","    user_node_normalized[user_id] = friend_nodes_normalized\n","    \n","    return user_node, user_node_normalized\n","\n","def genUserGraph(user_id, graph_type):\n","    if graph_type not in ['replies', 'mentions', 'quote_tweets']:\n","        print('Invalid graphType specified: ' + graph_type)\n","        return\n","    \n","    _,nodes = genNodesForGraph(user_id, graph_type)\n","    \n","    graph = nx.Graph()\n","    \n","    # Add the edges\n","    friend_nodes = nodes[user_id]\n","    users_friends_dict = {}\n","    for friend_id in friend_nodes:\n","        weight = friend_nodes[friend_id]\n","        \n","        # add each node to dict with it's label\n","        users_friends_dict[user_id] = 'user'\n","        users_friends_dict[friend_id] = 'friend'\n","        \n","        # add the edge\n","        graph.add_edge(user_id, friend_id, weight=weight)\n","        \n","    # label each node\n","    nx.set_node_attributes(graph, users_friends_dict, \"label\")\n","    \n","    # node features\n","    for node_id, node_data in graph.nodes(data=True):\n","        if node_data.get('label') == 'user':\n","            encoded_vector_tweets = all_encoded_user_tweets[node_id]\n","            node_data['type'] = 'user'\n","        elif node_data.get('label') == 'friend':\n","            encoded_vector_tweets = all_encoded_friend_tweets.get(node_id)\n","            ###### RESET THE LABEL TO USER SINCE GCN AND GAT ONLY WORK WITH ONE LABEL NODES\n","            node_data['label'] = 'user'\n","            node_data['type'] = 'friend'\n","\n","        node_data[\"feature\"] = encoded_vector_tweets\n","    \n","    stellargraph_data = StellarGraph.from_networkx(\n","        graph,\n","        node_features=\"feature\",\n","        node_type_default=\"user\",\n","        edge_type_default=graph_type)\n","    \n","#     nx.draw(graph, with_labels=False, node_color='red')\n","#     print(stellargraph_data.info())\n","    return stellargraph_data\n","\n","\n","def genMultiGraph(user_id):\n","    _,replies_nodes = genNodesForGraph(user_id, 'replies')\n","    _,mentions_nodes = genNodesForGraph(user_id, 'mentions')\n","    _,quote_tweets_nodes = genNodesForGraph(user_id, 'quote_tweets')\n","    \n","    graph = nx.MultiGraph()\n","    \n","    # Add the edges\n","    replies_friend_nodes = replies_nodes[user_id]\n","    mentions_friend_nodes = mentions_nodes[user_id]\n","    quote_tweets_friend_nodes = quote_tweets_nodes[user_id]\n","    replies_users_friends_dict = {}\n","    mentions_users_friends_dict = {}\n","    quote_tweets_users_friends_dict = {}\n","\n","    for friend_id in replies_friend_nodes:\n","        weight = replies_friend_nodes[friend_id]\n","        # add each node to dict with it's label\n","        replies_users_friends_dict[user_id] = 'user'\n","        replies_users_friends_dict[friend_id] = 'friend'\n","        # add the edge\n","        graph.add_edge(user_id, friend_id, weight=weight, label=\"replies\", color=\"r\")\n","\n","    for friend_id in mentions_friend_nodes:\n","        weight = mentions_friend_nodes[friend_id]\n","        # add each node to dict with it's label\n","        mentions_users_friends_dict[user_id] = 'user'\n","        mentions_users_friends_dict[friend_id] = 'friend'\n","        # add the edge\n","        graph.add_edge(user_id, friend_id, weight=weight, label=\"mentions\", color=\"g\")\n","\n","    for friend_id in quote_tweets_friend_nodes:\n","        weight = quote_tweets_friend_nodes[friend_id]\n","        # add each node to dict with it's label\n","        quote_tweets_users_friends_dict[user_id] = 'user'\n","        quote_tweets_users_friends_dict[friend_id] = 'friend'\n","        # add the edge\n","        graph.add_edge(user_id, friend_id, weight=weight, label=\"quote_tweets\", color=\"b\")\n","        \n","    # label each node\n","    nx.set_node_attributes(graph, replies_users_friends_dict, \"label\")\n","    nx.set_node_attributes(graph, mentions_users_friends_dict, \"label\")\n","    nx.set_node_attributes(graph, quote_tweets_users_friends_dict, \"label\")\n","    \n","    # node features\n","    for node_id, node_data in graph.nodes(data=True):\n","        if node_data.get('label') == 'user':\n","            encoded_vector_tweets = all_encoded_user_tweets[node_id]\n","            node_data['type'] = 'user'\n","        elif node_data.get('label') == 'friend':\n","            encoded_vector_tweets = all_encoded_friend_tweets.get(node_id)\n","            ###### RESET THE LABEL TO USER SINCE GCN AND GAT ONLY WORK WITH ONE LABEL NODES\n","            node_data['label'] = 'user'\n","            node_data['type'] = 'friend'\n","\n","        node_data[\"feature\"] = encoded_vector_tweets\n","    \n","    stellargraph_data = StellarGraph.from_networkx(\n","        graph,\n","        node_features=\"feature\",\n","        node_type_default=\"user\",\n","        edge_type_default=\"replies\")\n","    \n","#     nx.draw(graph, with_labels=False, node_color='red')\n","#     print(stellargraph_data.info())\n","    return stellargraph_data\n","\n","def getUsersGraphsDict():\n","  global train_users_graphs\n","  train_users_graphs = {}\n","  global test_users_graphs\n","  test_users_graphs = {}\n","\n","  global train_users_graph_labels\n","  train_users_graph_labels = []\n","  global test_users_graph_labels\n","  test_users_graph_labels = []\n","  global combined_graph_labels\n","  combined_graph_labels = []\n","\n","  # Train user graphs\n","  for user_id in graph_train_users['user_id'].values:\n","      train_users_graphs[user_id] = {}\n","      train_users_graph_labels.append(graph_train_users.loc[graph_train_users['user_id'] == user_id]['depressed'].values[0])\n","      train_users_graphs[user_id]['depressed'] = graph_train_users.loc[graph_train_users['user_id'] == user_id]['depressed'].values[0]\n","      train_users_graphs[user_id]['reply_graph'] = genUserGraph(user_id, 'replies')\n","      train_users_graphs[user_id]['mention_graph'] = genUserGraph(user_id, 'mentions')\n","      train_users_graphs[user_id]['quote_tweet_graph'] = genUserGraph(user_id, 'quote_tweets')\n","      train_users_graphs[user_id]['combined'] = genMultiGraph(user_id)\n","\n","  # Test user graphs\n","  for user_id in graph_test_users['user_id'].values:\n","      test_users_graphs[user_id] = {}\n","      test_users_graph_labels.append(graph_test_users.loc[graph_test_users['user_id'] == user_id]['depressed'].values[0])\n","      test_users_graphs[user_id]['depressed'] = graph_test_users.loc[graph_test_users['user_id'] == user_id]['depressed'].values[0]\n","      test_users_graphs[user_id]['reply_graph'] = genUserGraph(user_id, 'replies')\n","      test_users_graphs[user_id]['mention_graph'] = genUserGraph(user_id, 'mentions')\n","      test_users_graphs[user_id]['quote_tweet_graph'] = genUserGraph(user_id, 'quote_tweets')\n","      test_users_graphs[user_id]['combined'] = genMultiGraph(user_id)\n","\n","  # Combine Labels\n","  combined_graph_labels = train_users_graph_labels + test_users_graph_labels\n","\n","def getUsersGraphsCombined():\n","  global train_replies_graphs\n","  train_replies_graphs = []\n","  global train_mentions_graphs\n","  train_mentions_graphs = []\n","  global train_quote_tweets_graphs\n","  train_quote_tweets_graphs = []\n","  global train_combined_graphs\n","  train_combined_graphs = []\n","  global test_replies_graphs\n","  test_replies_graphs = []\n","  global test_mentions_graphs\n","  test_mentions_graphs = []\n","  global test_quote_tweets_graphs\n","  test_quote_tweets_graphs = []\n","  global test_combined_graphs\n","  test_combined_graphs = []\n","\n","  global combined_replies_graphs\n","  combined_replies_graphs = []\n","  global combined_mentions_graphs\n","  combined_mentions_graphs = []\n","  global combined_quote_tweets_graphs\n","  combined_quote_tweets_graphs = []\n","  global combined_graphs\n","  combined_graphs = []\n","\n","  for user_graphs in train_users_graphs:\n","      # print(train_users_graphs[user_graphs]['reply_graph'].info())\n","      train_replies_graphs.append(train_users_graphs[user_graphs]['reply_graph'])\n","      train_mentions_graphs.append(train_users_graphs[user_graphs]['mention_graph'])\n","      train_quote_tweets_graphs.append(train_users_graphs[user_graphs]['quote_tweet_graph'])\n","      train_combined_graphs.append(train_users_graphs[user_graphs]['combined'])\n","\n","  for user_graphs in test_users_graphs:\n","      # print(test_users_graphs[user_graphs]['reply_graph'].info())\n","      test_replies_graphs.append(test_users_graphs[user_graphs]['reply_graph'])\n","      test_mentions_graphs.append(test_users_graphs[user_graphs]['mention_graph'])\n","      test_quote_tweets_graphs.append(test_users_graphs[user_graphs]['quote_tweet_graph'])\n","      test_combined_graphs.append(test_users_graphs[user_graphs]['combined'])\n","\n","  # Combine\n","  combined_replies_graphs = train_replies_graphs + test_replies_graphs\n","  combined_mentions_graphs = train_mentions_graphs + test_mentions_graphs\n","  combined_quote_tweets_graphs = train_quote_tweets_graphs + test_quote_tweets_graphs\n","  combined_graphs = train_combined_graphs + test_combined_graphs\n","\n","  # Summaries\n","  replies_summary = pd.DataFrame(\n","      [(g.number_of_nodes(), g.number_of_edges()) for g in combined_replies_graphs],\n","      columns=[\"nodes\", \"edges\"],\n","  )\n","  mentions_summary = pd.DataFrame(\n","      [(g.number_of_nodes(), g.number_of_edges()) for g in combined_mentions_graphs],\n","      columns=[\"nodes\", \"edges\"],\n","  )\n","  quote_tweets_summary = pd.DataFrame(\n","      [(g.number_of_nodes(), g.number_of_edges()) for g in combined_quote_tweets_graphs],\n","      columns=[\"nodes\", \"edges\"],\n","  )\n","  combined_summary = pd.DataFrame(\n","      [(g.number_of_nodes(), g.number_of_edges()) for g in combined_graphs],\n","      columns=[\"nodes\", \"edges\"],\n","  )\n","\n","  global replies_graph_node_num\n","  replies_graph_node_num = np.max(replies_summary.nodes)\n","  global mentions_graph_node_num\n","  mentions_graph_node_num = np.max(mentions_summary.nodes)\n","  global quote_tweets_graph_node_num\n","  quote_tweets_graph_node_num = np.max(quote_tweets_summary.nodes)\n","  global combined_graph_node_num\n","  combined_graph_node_num = np.max(combined_summary.nodes)\n","\n","  print(\"Replies Graph Stats\")\n","  print(replies_summary.describe().round(1))\n","  print(\"\\nMentions Graph Stats\")\n","  print(mentions_summary.describe().round(1))\n","  print(\"\\nQuote Tweets Graph Stats\")\n","  print(quote_tweets_summary.describe().round(1))\n","  print(\"\\nCombined Graph Stats\")\n","  print(combined_summary.describe().round(1))"],"id":"3eb18e08","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9HI_41XiTaHP"},"source":["# Graph model functions"],"id":"9HI_41XiTaHP"},{"cell_type":"markdown","metadata":{"id":"02a51dc2"},"source":["### Prepare graph generator\n","\n","To feed data to the `tf.Keras` model that we will create later, we need a data generator. For supervised graph classification, we create an instance of `StellarGraph`'s `PaddedGraphGenerator` class. Note that `graphs` is a list of `StellarGraph` graph objects."],"id":"02a51dc2"},{"cell_type":"code","metadata":{"id":"7a7cd840","executionInfo":{"status":"ok","timestamp":1637014869894,"user_tz":480,"elapsed":10,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def prepGraphGens():\n","  replies_generator_gcn = PaddedGraphGenerator(graphs=combined_replies_graphs)\n","  mentions_generator_gcn = PaddedGraphGenerator(graphs=combined_mentions_graphs)\n","  quote_tweets_generator_gcn = PaddedGraphGenerator(graphs=combined_quote_tweets_graphs)\n","  combined_generator_gcn = PaddedGraphGenerator(graphs=combined_graphs)\n","# generator_gcn = FullBatchNodeGenerator(g)\n","\n","  return replies_generator_gcn, mentions_generator_gcn, quote_tweets_generator_gcn, combined_generator_gcn"],"id":"7a7cd840","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIs095TwUuDh"},"source":["### Misc"],"id":"eIs095TwUuDh"},{"cell_type":"code","metadata":{"id":"fcd571b9","executionInfo":{"status":"ok","timestamp":1637014870124,"user_tz":480,"elapsed":237,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def get_generators_flow(generator, batch_size):\n","    # We already have our train and test users split. The first 344 users are for training\n","    # and the rest 148 users are for testing\n","    # from 0 to 343 = 344 train users\n","    train_index = list(range(0, len(graph_train_users)))\n","    # from 344 to 492 = 148 train users\n","    test_index = list(range(len(graph_train_users), len(graph_train_users)+len(graph_test_users)))\n","\n","    # y = tf.keras.utils.to_categorical(combined_graph_labels, num_classes=2)\n","\n","    train_gen_flow = generator.flow(\n","        train_index, targets=pd.DataFrame(combined_graph_labels).iloc[train_index].values, batch_size=batch_size\n","    )\n","    test_gen_flow = generator.flow(\n","        test_index, targets=pd.DataFrame(combined_graph_labels).iloc[test_index].values, batch_size=batch_size\n","    )\n","\n","    return train_gen_flow, test_gen_flow"],"id":"fcd571b9","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLU_EnMbKca4"},"source":["### Create the Keras graph classification model\n","\n","We are now ready to create a `tf.Keras` graph classification model using `StellarGraph`'s `GraphClassification` class together with standard `tf.Keras` layers, e.g., `Dense`. \n","\n","The input is the graph represented by its adjacency and node features matrices. The first two layers are Graph Convolutional with each layer having 64 units and `relu` activations. The next layer is a mean pooling layer where the learned node representation are summarized to create a graph representation. The graph representation is input to two fully connected layers with 32 and 16 units respectively and `relu` activations. The last layer is the output layer with a single unit and `sigmoid` activation."],"id":"BLU_EnMbKca4"},{"cell_type":"code","metadata":{"id":"7bRTAL3oKB5j","executionInfo":{"status":"ok","timestamp":1637015866468,"user_tz":480,"elapsed":152,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def create_graph_classification_model_gcn(generator, k):\n","    gc_model = GCNSupervisedGraphClassification(\n","        layer_sizes=[64, 64],\n","        activations=[\"relu\", \"relu\"],\n","        generator=generator,\n","        dropout=0.5,\n","    )\n","    x_inp, x_out = gc_model.in_out_tensors()\n","    predictions = Dense(units=32, activation=\"relu\")(x_out)\n","    predictions = Dense(units=16, activation=\"relu\")(predictions)\n","    predictions = Dense(units=1, activation=\"sigmoid\")(predictions)\n","\n","    # Let's create the Keras model and prepare it for training\n","    model = Model(inputs=x_inp, outputs=predictions)\n","    # fpr, tpr, thresholds = metrics.roc_curve(test_users_graph_labels, predictions, pos_label=2)\n","    # model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=[metrics.auc(fpr, tpr), \"acc\"])\n","    model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=[\"acc\"])\n","\n","    return model, predictions, x_inp, x_out\n","\n","def create_graph_classification_model_dgcnn(generator, k, layer_sizes, activations):\n","    gc_model = DeepGraphCNN(\n","        layer_sizes=layer_sizes,\n","        activations=activations,\n","        k=k,\n","        generator=generator,\n","        dropout=0.5,\n","    )\n","    x_inp, x_out = gc_model.in_out_tensors()\n","    x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n","    x_out = MaxPool1D(pool_size=2)(x_out)\n","    # x_out = Conv1D(filters=32, kernel_size=3, strides=1)(x_out)\n","    x_out = Flatten()(x_out)\n","    x_out = Dense(units=128, activation=\"relu\")(x_out)\n","    x_out = Dropout(rate=0.5)(x_out)\n","    predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n","\n","    # Let's create the Keras model and prepare it for training\n","    model = Model(inputs=x_inp, outputs=predictions)\n","    # fpr, tpr, thresholds = metrics.roc_curve(test_users_graph_labels, predictions, pos_label=2)\n","    # model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=[metrics.auc(fpr, tpr), \"acc\"])\n","    model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=[\"acc\"])\n","\n","    return model, predictions, x_inp, x_out\n","\n","def train_fold(model, train_gen, test_gen):\n","  epochs = 300\n","  es = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=20, verbose=1, restore_best_weights=True, mode='auto')\n","\n","  history = model.fit(\n","      train_gen, epochs=epochs, validation_data=test_gen, verbose=0, callbacks=[es],\n","  )\n","  return model, history\n","\n","def trainModel(generator, k, layer_sizes, activations):\n","  print(\"K value: {}\".format(k))\n","\n","  train_gen_flow, test_gen_flow = get_generators_flow(generator=generator, batch_size=15)\n","\n","  model, predictions, x_inp, x_out = create_graph_classification_model_dgcnn(generator, k, layer_sizes, activations)\n","  display(model.summary())\n","\n","  model, history = train_fold(model, train_gen_flow, test_gen_flow)\n","\n","  return model, history, train_gen_flow, test_gen_flow, predictions, x_inp, x_out"],"id":"7bRTAL3oKB5j","execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e0S5KNxRWWsc"},"source":["### Stats"],"id":"e0S5KNxRWWsc"},{"cell_type":"code","metadata":{"id":"vhcuc93loAnV","executionInfo":{"status":"ok","timestamp":1637014870128,"user_tz":480,"elapsed":18,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def show_acc_stats(title, model, history, test_gen):\n","    stats = {}\n","    # calculate performance on the test data and return along with history\n","    test_metrics = model.evaluate(test_gen, verbose=0)\n","    # test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n","    \n","    print(\"\\n{} Test Set Metrics:\".format(title))\n","    for name, val in zip(model.metrics_names, test_metrics):\n","        print(\"\\t{}: {:0.4f}\".format(name, val))\n","        stats[name] = val\n","\n","    return stats"],"id":"vhcuc93loAnV","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6fWTAQe1b7U"},"source":["#### AU-ROC\n","Let's use the trained model to make a prediction for each graph.\n","\n","Then, select only the predictions for the graphs in the test set and calculate the AU-ROC as another performance metric in addition to the accuracy shown above."],"id":"P6fWTAQe1b7U"},{"cell_type":"code","metadata":{"id":"1Q9OrZWm1egD","executionInfo":{"status":"ok","timestamp":1637014870131,"user_tz":480,"elapsed":20,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def get_auroc_stats(name, pred):\n","  # replies_auc, replies_pr, replies_f_score = evaluate_preds(true=test_users_graph_labels, pred=pred_replies)\n","  model_auc = plot_roc(pred, test_users_graph_labels)\n","  print(\"The {} AUC on the test set: {}\".format(name, model_auc))\n","\n","  return model_auc"],"id":"1Q9OrZWm1egD","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJbgF-jn-eRa"},"source":["#### Confusion Matrix"],"id":"WJbgF-jn-eRa"},{"cell_type":"code","metadata":{"id":"ThzQnUgu-khA","executionInfo":{"status":"ok","timestamp":1637014870131,"user_tz":480,"elapsed":18,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def getConfusionMatrix(name, pred_bin):\n","  cm = confusion_matrix(np.array(test_users_graph_labels), pred_bin)\n","  print(cm)\n","  print('Plotting {} confusion matrix'.format(name))\n","  plt.figure()\n","  plot_confusion_matrix(cm, [\"healthy\", \"depressed\"])\n","  plt.show()\n","\n","  print(classification_report(np.array(test_users_graph_labels), pred_bin))"],"id":"ThzQnUgu-khA","execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsUaxw_8ffav"},"source":[""],"id":"dsUaxw_8ffav"},{"cell_type":"code","metadata":{"id":"7CWJTItsfeaB","executionInfo":{"status":"ok","timestamp":1637014870133,"user_tz":480,"elapsed":18,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}}},"source":["def getStats(name, pred_bin):\n","  precission = precision_score(np.array(test_users_graph_labels), pred_bin)\n","  recall = recall_score(np.array(test_users_graph_labels), pred_bin)\n","  f1 = f1_score(np.array(test_users_graph_labels), pred_bin)\n","  print(\"{} Precision score: {}\".format(name, precission))\n","  print(\"{} Recall score: {}\".format(name, recall))\n","  print(\"{} F1 Score: {}\".format(name, f1))\n","\n","  return precission, recall, f1"],"id":"7CWJTItsfeaB","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PftawqgwTeLs"},"source":["# Run model loop"],"id":"PftawqgwTeLs"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1gzroaFvs9K2l__fRvD4xygcJWC0Ecf76"},"id":"66Cprcqj62nO","executionInfo":{"status":"ok","timestamp":1637020790327,"user_tz":480,"elapsed":3671778,"user":{"displayName":"Ivan Mihov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYA03_KkeHxVX7jX6EOukE-va0taEQLjC6mbyhArw=s64","userId":"03661985643310973482"}},"outputId":"19dc413b-6887-4f90-a228-dd0395dc6d96"},"source":["# The total number of friends 0 = all friends\n","num_friends_list = [0]\n","# The number of tweets - most recent. 0 means no limit.\n","num_tweets_list = [50]\n","\n","\n","# lstm_layers = [1,2]\n","lstm_layers = [1]\n","# word_embedding_size_list: 100,200,300\n","word_embedding_size_list = [100]\n","# user_embedding_size_list: 50,100,200\n","user_embedding_size_list = [200]\n","\n","# DGCNN Params\n","# k\n","k_list = [5,10,15,35]\n","# l\n","layer_sizes_list = [\n","  [32, 32],\n","]\n","# j\n","activations_list = [\n","  [\"tanh\", \"tanh\"],\n","]\n","\n","itteration = 0\n","\n","stats = {}\n","\n","for lstm_num in lstm_layers:\n","    for word_embedding_size in word_embedding_size_list:\n","        for user_embedding_size in user_embedding_size_list:\n","            for num_friends in num_friends_list:\n","                for num_tweets in num_tweets_list:\n","                    for layer_sizes_idx, layer_sizes in enumerate(layer_sizes_list):\n","                        for k in k_list:\n","\n","                            # only have unlimitted tweets for num_friends = 3\n","                            if (num_friends != 3 or word_embedding_size != 100 or user_embedding_size != 200) and num_tweets == 0:\n","                              continue\n","      \n","                            itteration += 1\n","                            print(\"\\n\\nItteration: {}\".format(itteration))\n","                            print(\"#####################################################\")\n","                            print(\"Friends Count: {}\".format(num_friends))\n","                            print(\"Tweet Count: {}\".format(num_tweets))\n","      \n","                            print(\"LSTM count: {}\".format(lstm_num))\n","                            print(\"Word embeddings size: {}\".format(word_embedding_size))\n","                            print(\"User embeddings size: {}\".format(user_embedding_size))\n","\n","                            print(\"\\nDGCNN Params:\")\n","                            print(\"Layer Sizes (l): {}\".format(layer_sizes))\n","                            print(\"Activations (j): {}\".format(activations_list[layer_sizes_idx]))\n","                            print(\"Nodes after SortPooling (k): {}\".format(k))\n","                            print(\"#####################################################\\n\")\n","      \n","                            # Define the constants\n","                            defineGlobalConstants(num_friends_local = num_friends, num_tweets_local = num_tweets)\n","      \n","                            # Get the stats\n","                            df_replies_stats = pd.read_hdf(hdf_stats_reply_path, key=hdf_stats_key)\n","                            df_mentions_stats = pd.read_hdf(hdf_stats_mention_path, key=hdf_stats_key)\n","                            df_quote_tweets_stats = pd.read_hdf(hdf_stats_quote_tweets_path, key=hdf_stats_key)\n","                            print(\"Replies Stats Shape: {}\".format(df_replies_stats.shape))\n","                            print(\"Mentions Stats Shape: {}\".format(df_mentions_stats.shape))\n","                            print(\"Quote Tweets Stats Shape: {}\".format(df_quote_tweets_stats.shape))\n","      \n","                            # Get the encoded tweets\n","                            all_encoded_user_tweets = genEncodedTweets('user', lstm_num, word_embedding_size, user_embedding_size)\n","                            all_encoded_friend_tweets = genEncodedTweets('friend', lstm_num, word_embedding_size, user_embedding_size)\n","                            print(\"User Node Count: {}\".format(len(all_encoded_user_tweets)))\n","                            print(\"Friends Unique Node Count: {}\".format(len(all_encoded_friend_tweets)))\n","\n","                            # k = len(all_encoded_friend_tweets)+1\n","                            # print(\"TEST K: {}\".format(k))\n","      \n","                            # Get the train/test users\n","                            graph_train_users, graph_test_users = loadCombinedUsers()\n","                            print(\"Total users: {}\".format(len(graph_train_users)+len(graph_test_users)))\n","                            print(\"Total of {} train users\".format(len(graph_train_users)))\n","                            print(\"Total of {} test users\".format(len(graph_test_users)))\n","      \n","                            # Get a dictionary for each train/test user containing their graphs\n","                            getUsersGraphsDict()\n","                            # Train graph labels stats\n","                            print(\"Train labels\")\n","                            print(pd.Series(train_users_graph_labels).value_counts().to_frame())\n","                            # Test graph labels stats\n","                            print(\"Test labels\")\n","                            print(pd.Series(test_users_graph_labels).value_counts().to_frame())\n","      \n","                            # Get the combined user graphs\n","                            getUsersGraphsCombined()\n","      \n","                            # Draw a sample from the train replies graph\n","                            print(\"Sample reply graph\")\n","                            plt.figure(figsize=(10,10))\n","                            draw_networkx(train_replies_graphs[0].to_networkx(), node_size=2000, edge_color='grey')\n","      \n","                            # Get the generators\n","                            replies_generator_gcn, mentions_generator_gcn, quote_tweets_generator_gcn, combined_generator_gcn = prepGraphGens()\n","      \n","                            ##### TRAIN\n","                            # Replies model\n","                            replies_model, replies_history, replies_train_gen, replies_test_gen, replies_predictions, replies_x_inp, replies_x_out = trainModel(replies_generator_gcn, k, layer_sizes, activations_list[layer_sizes_idx])\n","                            # Mentions Model\n","                            mentions_model, mentions_history, mentions_train_gen, mentions_test_gen, mentions_predictions, mentions_x_inp, mentions_x_out = trainModel(mentions_generator_gcn, k, layer_sizes, activations_list[layer_sizes_idx])\n","                            # Quote Tweets Model\n","                            quote_tweets_model, quote_tweets_history, quote_tweets_train_gen, quote_tweets_test_gen, quote_tweets_predictions, quote_tweets_x_inp, quote_tweets_x_out = trainModel(quote_tweets_generator_gcn, k, layer_sizes, activations_list[layer_sizes_idx])\n","                            # Combined Model\n","                            combined_model, combined_history, combined_train_gen, combined_test_gen, combined_predictions, combined_x_inp, combined_x_out = trainModel(combined_generator_gcn, k, layer_sizes, activations_list[layer_sizes_idx])\n","      \n","                            ##### Predictions\n","                            replies_pred = replies_model.predict(replies_test_gen)\n","                            mentions_pred = mentions_model.predict(mentions_test_gen)\n","                            quote_tweets_pred = quote_tweets_model.predict(quote_tweets_test_gen)\n","                            combined_pred = combined_model.predict(combined_test_gen)\n","      \n","                            replies_pred_list = []\n","                            for i in range(len(replies_pred)):\n","                              replies_pred_list.append(np.max(replies_pred[i]))\n","                            mentions_pred_list = []\n","                            for i in range(len(mentions_pred)):\n","                              mentions_pred_list.append(np.max(mentions_pred[i]))\n","                            quote_tweets_pred_list = []\n","                            for i in range(len(quote_tweets_pred)):\n","                              quote_tweets_pred_list.append(np.max(quote_tweets_pred[i]))\n","                            combined_pred_list = []\n","                            for i in range(len(combined_pred)):\n","                              combined_pred_list.append(np.max(combined_pred[i]))\n","      \n","                            replies_pred_bin = predToBinPred(replies_pred_list, threshold=0.5)\n","                            mentions_pred_bin = predToBinPred(mentions_pred_list, threshold=0.5)\n","                            quote_tweets_pred_bin = predToBinPred(quote_tweets_pred_list, threshold=0.5)\n","                            combined_pred_bin = predToBinPred(combined_pred_list, threshold=0.5)\n","           \n","                            ##### EVAL\n","                            # Accuracy\n","                            stats_replies = show_acc_stats(\"Replies\", replies_model, replies_history, replies_test_gen)\n","                            stats_mentions = show_acc_stats(\"Mentions\", mentions_model, mentions_history, mentions_test_gen)\n","                            stats_quote_tweets = show_acc_stats(\"Quote Tweets\", quote_tweets_model, quote_tweets_history, quote_tweets_test_gen)\n","                            stats_combined = show_acc_stats(\"Combined\", combined_model, combined_history, combined_test_gen)\n","                            print('Replies Stats Graph')\n","                            sg.utils.plot_history(replies_history)\n","                            print('Mentions Stats Graph')\n","                            sg.utils.plot_history(mentions_history)\n","                            print('Quote Tweets Stats Graph')\n","                            sg.utils.plot_history(quote_tweets_history)\n","                            print('Combined Stats Graph')\n","                            sg.utils.plot_history(combined_history)\n","      \n","                            # AU-ROC\n","                            replies_auc = get_auroc_stats(\"Replies\", replies_pred_list)\n","                            mentions_auc = get_auroc_stats(\"Mentions\", mentions_pred_list)\n","                            quote_tweets_auc = get_auroc_stats(\"Quote-Tweets\", quote_tweets_pred_list)\n","                            combined_auc = get_auroc_stats(\"Combined\", combined_pred_list)\n","      \n","                            # Precision, Recall, F1\n","                            replies_precission, replies_recall, replies_f1 = getStats(\"Replies\", replies_pred_bin)\n","                            mentions_precission, mentions_recall, mentions_f1 = getStats(\"Mentions\", mentions_pred_bin)\n","                            quote_tweets_precission, quote_tweets_recall, quote_tweets_f1 = getStats(\"Quote-Tweets\", quote_tweets_pred_bin)\n","                            combined_precission, combined_recall, combined_f1 = getStats(\"Combined\", combined_pred_bin)\n","      \n","                            # Save all stats\n","                            if lstm_num not in stats:\n","                              stats[lstm_num] = {}\n","                            if word_embedding_size not in stats[lstm_num]:\n","                              stats[lstm_num][word_embedding_size] = {}\n","                            if user_embedding_size not in stats[lstm_num][word_embedding_size]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size] = {}\n","                            if num_friends not in stats[lstm_num][word_embedding_size][user_embedding_size]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends] = {}\n","                            if num_tweets not in stats[lstm_num][word_embedding_size][user_embedding_size][num_friends]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets] = {}\n","                            if 'replies' not in stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['replies'] = {}\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['replies']['auc'] = replies_auc\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['replies']['precission'] = replies_precission\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['replies']['recall'] = replies_recall\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['replies']['f1'] = replies_f1\n","                            if 'mentions' not in stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['mentions'] = {}\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['mentions']['auc'] = mentions_auc\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['mentions']['precission'] = mentions_precission\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['mentions']['recall'] = mentions_recall\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['mentions']['f1'] = mentions_f1\n","                            if 'quote_tweets' not in stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['quote_tweets'] = {}\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['quote_tweets']['auc'] = quote_tweets_auc\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['quote_tweets']['precission'] = quote_tweets_precission\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['quote_tweets']['recall'] = quote_tweets_recall\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['quote_tweets']['f1'] = quote_tweets_f1\n","                            if 'combined' not in stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]:\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['combined'] = {}\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['combined']['auc'] = combined_auc\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['combined']['precission'] = combined_precission\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['combined']['recall'] = combined_recall\n","                              stats[lstm_num][word_embedding_size][user_embedding_size][num_friends][num_tweets]['combined']['f1'] = combined_f1\n","      \n","                            # Confusion Matrix\n","                            getConfusionMatrix(\"Replies\", replies_pred_bin)\n","                            getConfusionMatrix(\"Mentions\", mentions_pred_bin)\n","                            getConfusionMatrix(\"Quote-Tweets\", quote_tweets_pred_bin)\n","                            getConfusionMatrix(\"Combined\", combined_pred_bin)\n","\n","pickle.dump(stats, open( graph_model_stats_file, \"wb\" ))\n"],"id":"66Cprcqj62nO","execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}